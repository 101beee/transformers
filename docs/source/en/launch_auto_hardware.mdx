<!--Copyright 2023 The HuggingFace Team. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Run Training or Inference on Remote Hardware (with Auto Setup)

This page demonstrates how to run model training or inference on remote, self-hosted hardware, with automatic
dependency and environment setup. You can develop fully locally on a Colab or local Python script, while execution
a remote cluster to take advantage of accelerators like GPUs or TPUs.

Remote hardware includes both on-demand clusters through cloud providers and existing clusters with SSH credentials.

This tutorial is also available as a Colab [here]().

## Instantiate Remote Cluster

Install Runhouse, which is used for creating a local Cluster object and sending remote function calls.

```python
import runhouse as rh
```

### On-Demand Clusters (AWS, Azure, GCP, LambdaLabs)

On-Demand clusters are cloud clusters that are spun up/down automatically for you.
For instructions on setting up cloud access for on-demand clusters, please refer to
[Hardware Setup](https://runhouse-docs.readthedocs-hosted.com/en/main/rh_primitives/cluster.html#hardware-setup).

```python
# Single GPU
gpu = rh.cluster(name="rh-cluster", instance_type="V100:1", provider="cheapest").up_if_not()
# Multi GPU
# gpu = rh.cluster(name="rh-cluster", instance_type="V100:4", provider="cheapest").up_if_not()
gpu.keep_warm(60)  # automatically terminate gpu after this long of inactivity (-1 to disable)
```

### On-Premise Cluster

To instantiate a BYO or on-prem cluster, you will need to provide the IP addresses and SSH credentials as follows.

```python
gpu = rh.cluster(
    ips=["<ip of the cluster>"], ssh_creds={"ssh_user": "...", "ssh_private_key": "<path_to_key>"}, name="rh-a10x"
)
```

# Inference

We'll start with inference. Below, we're defining an inference function to run on the remote hardware.

Load and initialize the gpt2 model and tokenizer, and then run inference on a sample input.

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

def gpt2(prompt, *args):
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    model = AutoModelForCausalLM.from_pretrained("gpt2")
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids
    output = model.generate(input_ids, max_length=100, do_sample=True)
    return tokenizer.decode(output[0], skip_special_tokens=True)
```

To set up the hardware environment to be able to seamlessly run the distributed launch function, first define the dependencies
necessary for running the training function with accelerate.

```python
reqs = ['./', 'torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu117']
```

Then, we can create a callable to launch our training function, setting the hardware/system that it is intended to be run on,
as well as the requirements for running the function.

```python
gpt2_gpu = rh.function(fn=gpt2).to(system=gpu, reqs=reqs)
```

Now we just call it.

```python
output = gpt2_gpu("Hello, I'm a language model, and I")
print(output)
```

## Training

# TODO